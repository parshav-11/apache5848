import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
import numpy as np
import matplotlib.ticker as ticker
from sklearn import preprocessing
%matplotlib inline

#notice: Disable all warnings 
import warnings
warnings.filterwarnings('ignore')

!wget -O loan_train.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/FinalModule_Coursera/data/loan_train.csv

#Load Data
df = pd.read_csv('loan_train.csv')
df.head()
df.shape

#Convert dataFrame into date time object
df['due_date'] = pd.to_datetime(df['due_date'])
df['effective_date'] = pd.to_datetime(df['effective_date'])
df.head()

#Letâ€™s see how many of each class is in our data set
df['loan_status'].value_counts()

# notice: installing seaborn might takes a few minutes
!pip install seaborn

#Let's plot some columns to underestand data better:
import seaborn as sns
bins = np.linspace(df.Principal.min(), df.Principal.max(), 10)
g = sns.FacetGrid(df, col="Gender", hue="loan_status", palette="Set1", col_wrap=2)
g.map(plt.hist, 'Principal', bins=bins, ec="k")
g.axes[-1].legend()
plt.show()
bins = np.linspace(df.age.min(), df.age.max(), 10)
g = sns.FacetGrid(df, col="Gender", hue="loan_status", palette="Set1", col_wrap=2)
g.map(plt.hist, 'age', bins=bins, ec="k")
g.axes[-1].legend()
plt.show()

#Preprocessing: Feature selection/extraction
#Let's look at the day of the week people get the loan
df['dayofweek'] = df['effective_date'].dt.dayofweek
bins = np.linspace(df.dayofweek.min(), df.dayofweek.max(), 10)
g = sns.FacetGrid(df, col="Gender", hue="loan_status", palette="Set1", col_wrap=2)
g.map(plt.hist, 'dayofweek', bins=bins, ec="k")
g.axes[-1].legend()
plt.show()

#We see that people who get the loan at the end of the week don't pay it off, so let's use Feature binarization to set a threshold value less than day 4
df['weekend'] = df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)
df.head()

#Convert categorical features to numerical values
df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)
df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)
df.head()

#One Hot Encoding
df.groupby(['education'])['loan_status'].value_counts(normalize=True)

#Features before One Hot Encoding
df[['Principal','terms','age','Gender','education']].head()

#Use one hot encoding technique to conver categorical varables to binary variables and append them to the feature Data Frame
Feature = df[['Principal','terms','age','Gender','weekend']]
Feature = pd.concat([Feature,pd.get_dummies(df['education'])], axis=1)
Feature.drop(['Master or Above'], axis = 1,inplace=True)
Feature.head()

#Feature Selection
#Let's define feature sets, X:
X = Feature
X[0:5]

#What are our lables?
y = df['loan_status'].values
y[0:5]

#Normalize Data
#Data Standardization give data zero mean and unit variance (technically should be done after train test split)
X= preprocessing.StandardScaler().fit(X).transform(X)
X[0:5]

#Classification
#Now, it is your turn, use the training set to build an accurate model. Then use the test set to report the accuracy of the model You should use the following 
#algorithm:
---->K Nearest Neighbor(KNN)
---->Decision Tree
---->Support Vector Machine
---->Logistic Regression
__ Notice:__
#You can go above and change the pre-processing, feature selection, feature-extraction, and so on, to make a better model.
#You should use either scikit-learn, Scipy or Numpy libraries for developing the classification algorithms.
#You should include the code of the algorithm in the following cells.

-------------------KNN-------------------
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4)
error_rate = []
for i in range(1,40):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i !=y_test))
    
    
plt.figure(figsize=(10,6))
plt.plot(range(1,40), error_rate, color = 'blue', linestyle = 'dashed', marker = 'o', markerfacecolor = 'red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.ylabel('Error Rate')
plt.xlabel('K')
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train,y_train)
pred_i = knn.predict(X_test)
print('CONFUSION_MATRIX :\n')
print(confusion_matrix(pred_i,y_test))
print('\n')
print('REPORT :\n')
print(classification_report(pred_i,y_test))
print('ACCURACY :')
metrics.accuracy_score(pred_i,y_test)


-------------------DecisionTree-------------------
from sklearn.tree import DecisionTreeClassifier
dc = DecisionTreeClassifier()
classification = dc.fit(X_train, y_train)
prediction = dc.predict(X_test)
prediction
print('CONFUSION_MATRIX :\n')
print(confusion_matrix(prediction,y_test))
print('REPORT :\n')
print(classification_report(prediction,y_test))
print('ACCURACY :\n')
metrics.accuracy_score(prediction,y_test)


-------------------SVM-------------------
from sklearn import svm
svc = svm.SVC(kernel='rbf')
svc.fit(X_train,y_train)
y_pred = svc.predict(X_test)
y_pred
print('CLASSIFICATION_REPORT :\n')
print(metrics.classification_report(y_pred,y_test))
print('CONFUSION MATRIX :\n')
print(metrics.confusion_matrix(y_pred,y_test))
print('ACCURACY :\n')
print(metrics.accuracy_score(y_pred,y_test))


-------------------Logistic Regression-------------------
from sklearn.linear_model import LogisticRegression
linear = LogisticRegression(fit_intercept = True)
linear.fit(X_train, y_train)
y_pred = linear.predict(X_test)
y_pred
print('CLASSIFICATION_REPORT :\n');
print(metrics.classification_report(y_pred,y_test))
print('CONFUSION_MATRIX :\n');
print(metrics.confusion_matrix(y_pred,y_test))
print('ACCURACY_SCORE :\n');
print(metrics.accuracy_score(y_pred,y_test))


#Model evaluation using  Test set
from sklearn.metrics import jaccard_similarity_score
from sklearn.metrics import f1_score
from sklearn.metrics import log_loss

#Download data and load dataset
!wget -O loan_test.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv
test_df = pd.read_csv('loan_test.csv')
test_df.head()

#Preprocessing
# convert date time
test_df['due_date'] = pd.to_datetime(test_df['due_date'])
test_df['effective_date'] = pd.to_datetime(test_df['effective_date'])
test_df['dayofweek'] = test_df['effective_date'].dt.dayofweek
# evaulate weekend field
test_df['weekend'] = test_df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)
# convert male to 0 and female to 1
test_df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)
# work out education level
test_feature = test_df[['Principal','terms','age','Gender','weekend']]
test_feature = pd.concat([test_feature,pd.get_dummies(test_df['education'])], axis=1)
test_feature.drop(['Master or Above'], axis = 1,inplace=True)
# normalize the test data
test_X = preprocessing.StandardScaler().fit(test_feature).transform(test_feature)
test_X[0:5]
# and target result
test_y = test_df['loan_status'].values
test_y[0:5]


-------------------Jaccard Score-------------------
# evaluate KNN
knn_yhat = knn.predict(test_X)
jc1 = (jaccard_similarity_score(test_y, knn_yhat))
# evaluate Decision Trees
dt_yhat = dc.predict(test_X)
jc2 = (jaccard_similarity_score(test_y, dt_yhat))
#evaluate SVM
svm_yhat = svc.predict(test_X)
jc3 = (jaccard_similarity_score(test_y, svm_yhat))
# evaluate Logistic Regression
lr_yhat = linear.predict(test_X)
jc4 = (jaccard_similarity_score(test_y, lr_yhat))
list_jc = [jc1, jc2, jc3, jc4]
list_jc


-------------------F1 Score-------------------
# evaluate KNN
fs1 = (f1_score(test_y, knn_yhat,average='weighted'))
# evaluate Desision Trees 
fs2 = (f1_score(test_y, dt_yhat, average='weighted'))
# evaluate SVM
fs3 = (f1_score(test_y, svm_yhat, average='weighted'))
# evaluate Logistic Regression
fs4 = (f1_score(test_y, lr_yhat, average='weighted'))
list_fs = [fs1, fs2, fs3, fs4]
list_fs
